\documentclass{InsightArticle}

\usepackage[dvips]{graphicx}
\usepackage{float}
\usepackage[hang]{subfigure}

\usepackage[dvips,
bookmarks,
bookmarksopen,
backref,
colorlinks,linkcolor={blue},citecolor={blue},urlcolor={blue},
]{hyperref}

\title{A Patch-Based Inpainting Framework}

% 
% NOTE: This is the last number of the "handle" URL that 
% The Insight Journal assigns to your paper as part of the
% submission process. Please replace the number "1338" with
% the actual handle number that you get assigned.
%
\newcommand{\IJhandlerIDnumber}{3250}

% Increment the release number whenever significant changes are made.
% The author and/or editor can define 'significant' however they like.
\release{0.00}

% At minimum, give your name and an email address.  You can include a
% snail-mail address if you like.

\author{David Doria}
\authoraddress{Rensselaer Polytechnic Institute, Troy NY}

\begin{document}

\IJhandlefooter{\IJhandlerIDnumber}


\ifpdf
\else
   %
   % Commands for including Graphics when using latex
   % 
   \DeclareGraphicsExtensions{.eps,.jpg,.gif,.tiff,.bmp,.png}
   \DeclareGraphicsRule{.jpg}{eps}{.jpg.bb}{`convert #1 eps:-}
   \DeclareGraphicsRule{.gif}{eps}{.gif.bb}{`convert #1 eps:-}
   \DeclareGraphicsRule{.tiff}{eps}{.tiff.bb}{`convert #1 eps:-}
   \DeclareGraphicsRule{.bmp}{eps}{.bmp.bb}{`convert #1 eps:-}
   \DeclareGraphicsRule{.png}{eps}{.png.bb}{`convert #1 eps:-}
\fi


\maketitle


\ifhtml
\chapter*{Front Matter\label{front}}
\fi

\begin{abstract}
\noindent
This document describes a system to fill a hole in an image by copying patches from elsewhere in the image. These patches should be a good continuation of the image outside the hole boundary into the hole. The implementation is very generic, allowing the develop to select or easily add new methods for the patch priority order, patch comparison function, and other parameters of the algorithm.

The ``basic'' algorithm is called ClassicalImpageInpainting and is based on the algorithm described in ``Object Removal by Exemplar-Based Inpainting'' (Criminisi et. al.).

The code is available here:
https://github.com/daviddoria/PatchBasedInpainting

\end{abstract}

\IJhandlenote{\IJhandlerIDnumber}

\tableofcontents
\section{Introduction}
This document describes a system to fill a hole in an image by copying patches from elsewhere in the image. These patches should be a good continuation of the hole boundary into the hole. The patch copying is done in an order which attempts to preserve linear structures in the image.

\section{Dependencies}
This code makes heavy use of multiple libraries:

\begin{itemize}
 \item VTK >= 6.0
 \item ITK >= 4.2
 \item Boost >= 1.51
 \item CMake >= 2.8.6
 \item Qt >= 4.8
\end{itemize}

The code is also organized into git submodules. These are included if you clone with:
\begin{verbatim}
git clone --recursive https://github.com/daviddoria/PatchBasedInpainting.git 
\end{verbatim}

or 

\begin{verbatim}
git clone https://github.com/daviddoria/PatchBasedInpainting.git
git submodule update --init --recursive
\end{verbatim}

The required submodules are:

\begin{itemize}
 \item Mask
 \item ITKHelpers (via Mask)
 \item Helpers (via Mask\\ITKHelpers)
 \item BoostHelpers
 \item CMakeHelpers
 \item ITKQtHelpers
 \item ITKVTKCamera
 \item ITKVTKHelpers
 \item QtHelpers
 \item VTKHelpers
\end{itemize}

\section{Terminology}
Throughout this document, the ``source region'' is the portion of the image which is known (is not part of the hole) at the beginning. The ``target region'' is the current hole to be filled.

\section{Basic Algorithm Overview}
The inputs to the algorithm consist of an image and a binary mask the same size as the image. We use a custom Mask class to describe the hole (https://github.com/daviddoria/Mask). Throughout this paper, we have colored the region in the input image corresponding to the hole bright green. This color irrelevant - we have done this only to make it obvious to tell if any part of the hole remains after inpainting (it should not), and for easier debugging to ensure these pixels are not used in any part of the computation. In practice, the input image need not be modified.

\section{Algorithm Synthetic Demonstration}
Figure \ref{fig:SyntheticDemonstration} shows a synthetic demonstration of the algorithm. The image consists of a black region (top) and a gray region (bottom).  This simple example is used for testing because we know the result to expect - the dividing line between the black region and gray region should be continued smoothly.

\begin{figure}[H]
\centering
\subfigure[Image to be filled. The region to be filled is shown in bright green.]
  {
  \includegraphics[width=0.3\linewidth]{images/BlackWhite}
  \label{fig:SyntheticDemonstration:ExampleInputImage}
  }
\subfigure[The mask of the region to inpaint.]
  {
  \includegraphics[width=0.3\linewidth]{images/BlackWhiteMask}
  \label{fig:SyntheticDemonstration:ExampleInputMask}
  }
\subfigure[The result of the inpainting.]
  {
  \includegraphics[width=0.3\linewidth]{images/BlackWhiteResult}
  \label{fig:SyntheticDemonstration:ExampleInputOutput}
  }
\caption{Synthetic Demonstration}
\label{fig:SyntheticDemonstration}
\end{figure}

\section{Algorithm Realistic Demonstration}
Figure \ref{fig:RealisticDemonstration} shows a real example of the algorithm. This result shows the typical quality of inpainting that the algorithm produces. 

\begin{figure}[H]
\centering
\subfigure[Image to be filled. The region to be filled is shown in bright green.]
  {
  \includegraphics[width=0.3\linewidth]{images/Bungee}
  \label{fig:RealisticDemonstration:ExampleInputImage}
  }
\subfigure[The mask of the region to inpaint.]
  {
  \includegraphics[width=0.3\linewidth]{images/BungeeMask}
  \label{fig:RealisticDemonstration:ExampleInputMask}
  }
\subfigure[The result of the inpainting. This took about 30 seconds on a P4 3GHz processor with a 206x308 image and a patch radius = 5.]
  {
  \includegraphics[width=0.3\linewidth]{images/BungeeResult}
  \label{fig:RealisticDemonstration:ExampleInputOutput}
  }
\caption{Realistic Demonstration}
\label{fig:RealisticDemonstration}
\end{figure}

\section{Algorithm Structure}
An overview of the algorithm is:
\begin{itemize}
  \item Initialize:
    \begin{itemize}
      \item Read an image and a binary mask. Non-zero pixels in the mask describe the hole to fill.
      \item Set the size of the patches which will be copied. (Typically an 11x11 patch (patch radius = 5) is used).
      \item Locate all patches of the image which are completely inside the image and completely in the source region. These are stored as an $std::vector<itk::ImageRegion<2> >$ named SourcePatches.
    \end{itemize}

  \item Main loop:
  \begin{itemize}
    \item Compute the priority of every pixel on the hole boundary (see Section \ref{subsec:AlgorithmDetails:Priorities})
    \item Determine the boundary pixel with the highest priority. We will call this the target pixel. The region centered at the target pixel and the size of the patches is called the target patch.
    \item Find the SourcePatch which best matches the portion of the target patch in the source region.
    \item Copy the corresponding portion of the source patch into the target region of the target patch.
    \item Repeat until the target region consists of zero pixels.
  \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%
\section{Algorithm Details}
\label{sec:AlgorithmDetails}

The two main parts of the algorithm are
\begin{enumerate}
 \item Determine the priority of each boundary pixel. This determines the order with which the hole is filled.
 \item Find the best matching patch to the patch around the pixel with the highest priority.
\end{enumerate}

%%%%%%%%%%%%%%%%%%
\section{Priority Functions}
\label{sec:PriorityFunctions}
\subsection{PriorityCriminisi}
The priority function is used to determine which target patch to fill next. The priority term described in \cite{criminis} is is given by the product of a Confidence term $C(p)$ and a Data term $D(p)$. This priority function attempts to both continue linear structures sooner rather than later, and fill patches where a larger number of the pixels in the patch are already filled.

\subsection{PriorityConfidence}
This priority function is the confidence term from the Criminisi priority function. Using this function essentially makes the algorithm fill patches from the outside of the hole and work its way inward.

\subsection{PriorityRandom}
This priority function selects a random target node to fill next. It is probably best to only use this ordering for debugging.

%%%%%%%%%%%%%%%%
\section{Patch Matching/Comparison/Difference}
\label{sec:PatchMatching}
We allow a ``patch'' to be a quite abstract concept. (In our research we have attempted to inpaint 3D data, so in this context a ``patch'' is no longer a simple grid of pixels). However, for a patch-based inpainting algorithm, there must always be the concept of ``comparing patches''. In the DifferenceFunctions folder, we provide many such comparisons.

\subsection{SumSquaredPixelDifference (SSD)}
This function computes the normalized sum of squared differences between every pixel in the valid region of the target patch and its corresponding pixel in the source patch. This is the standard difference function used in patch-based inpainting (e.g. \cite{criminisi}). This function is generic for any pixel type that has an operator[], but is specialized for pixels of type itk::CovariantVector. We also provide an explicitly unrolled version of this function, as since it is at the heart of the algorithm and the computational bottleneck, we have tried to do everything possible to ensure it runs as fast as possible.

\subsection{HSVSSD}
The standard SSD function is acceptable if the image is represented in a color space (like RGB) where each channel is ``non-wrapping''. That is, the values in the upper range of the channel (255) should be significantly different from the values in the lower range of the channel (0). This is not the case with the H channel of the HSV color space, so we must treat its cyclic nature specially. In this class, we treat the S and V channels as ``standard'' channels, and use a special difference functor for the H channel that takes into account that we are measuring an angular distance and that wrapping over the upper range (1) back into the lower range (0) does not indicate an enormous difference, but rather we handle it correctly.

%%%%%%%%%%%%%%%
\section{Implementation Details}
\label{sec:ImplementationDetails}

\subsection{Isophotes}
An isophotes is simply a gradient vector rotated by 90 degrees. It indicates the direction of ``same-ness'' rather than the direction of maximum difference. There is a small trick, however, to computing the isophotes. We originally tried to compute the isophotes using the following procedure:

\begin{itemize}
 \item Convert the RGB image to a grayscale image.
 \item Blur the grayscale image.
 \item Compute the gradient using itkGradientImageFilter.
 \item Rotate the resulting vectors by 90 degrees.
 \item Keep only the values in the source region.
\end{itemize}

This procedure produces the gradient magnitude map shown in Figure \ref{fig:ErroneousGradient}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.3\linewidth]{images/ErroneousGradient}
  \caption{Result of naively computing the image gradient.}
  \label{fig:ErroneousGradient}
\end{figure}

The high values of the gradient magnitude surrounding the target region are very troublesome. The resulting gradient magnitude image using this technique is sensitive to the choice of the pixel values in the target region, which we actually want to be a completely arbitrary choice (it should not affect anything). More importantly, the gradient plays a large part in the computation of the pixel priorities, and this computation is greatly disturbed by these erroneous values. Simply ignoring these boundary isophotes is not an option because the isophotes on the boundary are exactly the ones that are used in the computation of the Data term. To fix this problem, we immediately dilate the mask specified by the user. This allows us to compute the isophotes as described above, but now we have image information on both sides of the hole boundary, leading to a valid gradient everywhere we need it to be. Figure \ref{fig:ErrorneousGradientCorrection} shows the procedure for fixing this problem.

\begin{figure}[H]
\centering
\subfigure[Image to be filled. The target region is shown in green.]
  {
  \includegraphics[width=0.3\linewidth]{images/BlackWhite}
  \label{fig:ErrorneousGradientCorrection:InputImage}
  }
\subfigure[The dilated mask.]
  {
  \includegraphics[width=0.3\linewidth]{images/BlackWhiteDilatedMask}
  \label{fig:ErrorneousGradientCorrection:InputDilated}
  }
\subfigure[The gradient magnitude with pixels in the new (enlarged) target region set to zero.]
  {
  \includegraphics[width=0.3\linewidth]{images/MaskedGradientMagnitude}
  \label{fig:ErrorneousGradientCorrection:Output}
  }
\caption{Procedure for fixing the erroneous gradient problem.}
\label{fig:ErrorneousGradientCorrection}
\end{figure}

As you can see, this gradient magnitude image is exactly what we would expect.

\subsection{Boundary Normals}
There are two things to be careful with when computing the boundary normals: computing the normals only on the one pixel thick boundary, and using the correct side of the masked region as the boundary.

\subsubsection{Computing boundary normals only on the one pixel thick boundary}
If we compute the normals directly on the binary mask, the set of resulting vectors are too discretized to be of use. Therefore, we first blur the mask. However, the gradient of the blurred mask results in non-zero vectors in the gradient image in many more pixels (a ``thick'' boundary) than the gradient of the original mask (a single pixel ``thin'' boundary). Therefore, we must mask the gradient of the blurred mask to keep only the pixels which would have been non-zero in the original mask gradient.


\subsubsection{Using the correct side of the masked region as the boundary}
There are two potential boundaries that can be extracted from a masked region - the ``inner'' boundary and the ``outer'' boundary. As shown in Figure \ref{fig:BoundarySide}, the inner boundary (red) is composed of pixels originally on the white (masked) side of the blob, and the outer boundary (green) is composed of pixels originally on the black (unmasked) side of the blob. It is important that we use the outer boundary, because we need the boundary to be defined at the same pixels that we have image information, which is only in the source (black/unmasked) region.

\begin{figure}[H]
\centering
\subfigure[The inner boundary.]
  {
  \includegraphics[width=0.3\linewidth]{images/BlackWhiteMask}
  \label{fig:BoundarySide:Mask}
  }
\subfigure[Outer boundary (green) and inner boundary (red).]
  {
  \includegraphics[width=0.3\linewidth]{images/BothBoundaries}
  \label{fig:BoundarySide:BothBoundaries}
  }
\caption{Inner vs Outer Boundary of a Region}
\label{fig:BoundarySide}
\end{figure}

\subsubsection{Code Snippet}
The CriminisiInpainting class must be instantiated using the type of image to be inpainted. Then the patch radius must be set, the image and mask provided, and the Inpaint() function called.
\begin{verbatim}
  CriminisiInpainting<ImageType> Inpainting;
  Inpainting.SetPatchRadius(5);
  Inpainting.SetImage(imageReader->GetOutput());
  Inpainting.SetInputMask(maskReader->GetOutput());
  Inpainting.Inpaint();

  ImageType::Pointer result = Inpainting.GetResult();

\end{verbatim}

If you would like to see what happens at every step of the algorithm, you can use:
\begin{verbatim}
Inpainting.SetWriteIntermediateImages(true); 
\end{verbatim}
NOTE: Several images are output at each iteration - these files could be quite large!


%%%%%%%%%%%%%%%
\begin{thebibliography}{9}

	\bibitem{criminisi}
	  A. Criminisi, P. Perez, K. Toyama,
	  \emph{Object Removal by Exemplar-Based Inpainting}.
	  Computer Vision and Pattern Recognition 2003

\end{thebibliography}

\end{document}